{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hal Mimic 2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE+wDiTh0JnvHIdJ6/Ep5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkichaps/HAL-Mimic-2.0/blob/main/Hal_Mimic_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnI1KxcUj8Pe"
      },
      "source": [
        "# Hal Mimic 2.0\n",
        "\n",
        "This is an improvement over my existing Hal Mimic chatbot. This uses an Encoder-Decoder model along with an attention layer for better context responses compared to the previous chatbot which used two bi-directional LSTM layers.\n",
        "\n",
        "The model being used here was inspired by the Neural Machine Translation model used in the Deep Learning.ai Attention models course and has been tweaked to be used as a chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8U0-tz6vZeO"
      },
      "source": [
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "#Trax libs:\n",
        "!pip install trax sentencepiece t5\n",
        "import trax\n",
        "import sentencepiece as spm\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZZDuOqCZ4Zz"
      },
      "source": [
        "## Data pre-processing\n",
        "In this step we upload the Whatsapp txt chat records to the runtime environment and convert them into the data needed for training and eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlSqpCUiXfHY"
      },
      "source": [
        "#Removes emojis \n",
        "def deEmojify(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7HPXYqhaJmj"
      },
      "source": [
        "#Formats the whatsapp data in proper format. Similar function for different social media pages can be added.\n",
        "def format_data(file):\n",
        "  data_n = open(file).read()\n",
        "  data_n = deEmojify(data_n)\n",
        "  data_n = data_n.splitlines()\n",
        "  sentences = []\n",
        "  for item in data_n:\n",
        "    no = item[0:8]\n",
        "    try:\n",
        "      datetime_object = datetime.strptime(no, '%m/%d/%y')\n",
        "    except ValueError:\n",
        "      continue \n",
        "    l = item.split(\"-\",1)\n",
        "    del(l[0])\n",
        "    s = l[0]\n",
        "    f = s.split(\":\",1)\n",
        "    f[0] = f[0].strip()\n",
        "    f[1] = f[1].strip()  \n",
        "    sentences.append(f)    \n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2GBWXAWlBmm"
      },
      "source": [
        "In this step, Remember to enter your name wherever you see \\<Name\\>. Enter it in the same way it looks in the Whatsapp chat file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Il0fJVaPI_"
      },
      "source": [
        "#Splits data into the messages received and sent\n",
        "def split_data(data):  \n",
        "  sentences = []\n",
        "  labels = []\n",
        "  i = 0  \n",
        "  while i < len(data):    \n",
        "    if data[i][1] == '<Media omitted>':      #Removes <Media ommitted> from the chats\n",
        "      del(data[i])      \n",
        "      continue\n",
        "    if data[i][0] == '<Name>':     #As seen on the whatsapp data, put your name there\n",
        "      if i != 0 and data[i-1][0] == '<Name>':\n",
        "        labels[len(labels)-1] += \" \" + data[i][1]\n",
        "      else:\n",
        "        labels.append(data[i][1])\n",
        "    else:\n",
        "      if i != 0 and data[i-1][0] != '<Name>':\n",
        "        sentences[len(sentences)-1] += \" \" + data[i][1]\n",
        "      else:\n",
        "        sentences.append(data[i][1])\n",
        "    i+=1\n",
        "  if len(sentences) > len(labels):\n",
        "    labels.append('okay')     #Arbitrary word to keep data of the same size\n",
        "  elif len(sentences) < len(labels):\n",
        "    sentences.append('okay')\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXTHO4aclk9d"
      },
      "source": [
        "Here put this file name of the whatsapp chat text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_OlxsNMaNA9"
      },
      "source": [
        "#Add the whatsapp text files here (how many ever chats you need)\n",
        "f_sentences = []\n",
        "f_sentences.append(format_data('<file1>'))\n",
        "f_sentences.append(format_data('<file2>'))  \n",
        "f_sentences.append(format_data('<file3>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlEFTIMtai9N"
      },
      "source": [
        "final_sentences = []\n",
        "final_labels = []\n",
        "mydict = {}\n",
        "for item in f_sentences:    \n",
        "  s, l = split_data(item)  \n",
        "  print(len(s),len(l))\n",
        "  for i in range(len(s)):\n",
        "    if mydict.get(l[i],False):\n",
        "      continue\n",
        "    mydict[l[i]] = True\n",
        "    final_sentences.append(s[i])\n",
        "    final_labels.append(l[i])\n",
        "print(final_sentences)\n",
        "print(final_labels)\n",
        "print(len(final_sentences))\n",
        "print(len(final_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSJRlZl2DNVG"
      },
      "source": [
        "# Removes links in the dialogue\n",
        "def removelinks(sent):\n",
        "  a = sent.split(\" \")\n",
        "  length = len(a)\n",
        "  i = 0\n",
        "  while i < length:\n",
        "    if 'https://' in a[i]:\n",
        "      a.pop(i)\n",
        "      length -= 1\n",
        "    else:\n",
        "      i += 1\n",
        "  return \" \".join(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb50fSRvl02q"
      },
      "source": [
        "# Creates a Corpus.txt that contains all the words to develop the vocabulary\n",
        "file1 = open(\"Corpus.txt\",\"a\")\n",
        "for l in final_sentences:\n",
        "  if 'https://' in l:\n",
        "    l = removelinks(l)\n",
        "  file1.write(l + '\\n')\n",
        "\n",
        "for l in final_labels:\n",
        "  if 'https://' in l:\n",
        "    l = removelinks(l)\n",
        "  file1.write(l + '\\n')\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz5zaqhJcBsw"
      },
      "source": [
        "#See a sample interaction:\n",
        "index = random.randint(0,len(final_labels))\n",
        "print(\"You:\",final_labels[index])\n",
        "print(\"Them:\",final_sentences[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTif4HfDvPNc"
      },
      "source": [
        "## Processing Dialogues\n",
        "\n",
        "Now that we have the dialogues as _sentences_ and _labels_ we need to convert these data into the input for our model. We will do this by tokenizing our sentences and putting them into batches to train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAfF88a733vr"
      },
      "source": [
        "# Creating a generator for our model\n",
        "# Switching sentences and labels as sentences is the input and label is the target\n",
        "def stream(labels, sentences):\n",
        "  for i in range(len(labels)):\n",
        "    yield (sentences[i],labels[i])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4C67Pm35gEE"
      },
      "source": [
        "# Splitting our data into train and eval\n",
        "cutoff = int(len(final_labels)*0.9)\n",
        "train_labels = final_labels[:cutoff]\n",
        "train_sentences = final_sentences[:cutoff]\n",
        "eval_labels = final_labels[cutoff:]\n",
        "eval_sentences = final_sentences[cutoff:]\n",
        "print(\"Length of training data:\",len(train_labels))\n",
        "print(\"Length of eval data:\",len(eval_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWEQo3TO6zWC"
      },
      "source": [
        "# Creating the train and eval streams\n",
        "train_stream = stream(train_labels, train_sentences)\n",
        "eval_stream = stream(eval_labels, eval_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F11CvrlVvsI_"
      },
      "source": [
        "# Building the vocabulary file from Corpus.txt\n",
        "spm.SentencePieceTrainer.train(input='Corpus.txt', model_type='bpe', model_prefix='wa', vocab_size=30551) #Vocab size here is one I have assigned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezhX0SHf7DVy"
      },
      "source": [
        "# vocabulary filename\n",
        "VOCAB_FILE = 'wa.model'\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = '.'\n",
        "VOCAB_TYPE='sentencepiece'\n",
        "\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, vocab_type=VOCAB_TYPE)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, vocab_type=VOCAB_TYPE)(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbG6RRzenp1V"
      },
      "source": [
        "Here we are using '1' as the EOS token for every dialogue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qITxuDHl71m2"
      },
      "source": [
        "# Append EOS at the end of each sentence.\n",
        "\n",
        "# Integer assigned as end-of-sentence (EOS)\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEJ1W5DinzS6"
      },
      "source": [
        "Here we filter long sentences so that we don't run out of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFQ9WhnJ8Ir4"
      },
      "source": [
        "# length_keys=[0, 1] means we filter both input and target sentences, so the max tokens will be 256 for training and 512 for eval\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
        "\n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)    \n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print('Single tokenized example input:', train_input)\n",
        "print('Single tokenized example target:', train_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxhDA1xwokUR"
      },
      "source": [
        "These are helper functions to tokenize and detokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LafDFct4-KpU"
      },
      "source": [
        "#Encodes string into array of integers\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None, vocab_type=None):    \n",
        "        \n",
        "    EOS = 1\n",
        "    \n",
        "    # trax.data.tokenize takes streams and returns streams    \n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir, vocab_type=vocab_type))\n",
        "        \n",
        "    inputs = list(inputs) + [EOS]\n",
        "    \n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    \n",
        "    return batch_inputs\n",
        "\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None, vocab_type=None):      \n",
        "    # Remove the dimensions of size 1\n",
        "    integers = list(np.squeeze(integers))\n",
        "    \n",
        "    EOS = 1\n",
        "    \n",
        "    # Removing the EOS to get the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)]     \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir, vocab_type=vocab_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5JK1rk_C1Y"
      },
      "source": [
        "# Example sentences\n",
        "print('Single detokenized example input:', detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR,vocab_type=VOCAB_TYPE))\n",
        "print('Single detokenized example target:', detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR,vocab_type=VOCAB_TYPE))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0pgAnnFeDZ"
      },
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqPk-Tg0FpUF"
      },
      "source": [
        "## Exploring the data\n",
        "\n",
        "Now that we have processed the input, we can see some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_EoRkrfFqyR"
      },
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyiyxw_oFxfn"
      },
      "source": [
        "# pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print('Input sentence: \\n', detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, vocab_type=VOCAB_TYPE), '\\n')\n",
        "print('Tokenized Input sentence: \\n ', input_batch[index], '\\n')\n",
        "print('Target sentence: \\n', detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, vocab_type=VOCAB_TYPE), '\\n')\n",
        "print('Tokenized target sentence: \\n', target_batch[index], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c_Qyi_dOJYo"
      },
      "source": [
        "## The Model\n",
        "\n",
        "The model uses an encoder-decoder architecture of LSTMs with an attention layer in between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVqwLPVoKyjO"
      },
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\n",
        "    activations that will be the keys and values for attention.\n",
        "    \n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    Returns:\n",
        "        tl.Serial: The input encoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    input_encoder = tl.Serial( \n",
        "                \n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(input_vocab_size,d_model),\n",
        "        \n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
        "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]        \n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQUed3grOSEu"
      },
      "source": [
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
        "    activations that are used as queries in attention.\n",
        "    \n",
        "    Args:\n",
        "        mode: str: 'train' or 'eval'\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    Returns:\n",
        "        tl.Serial: The pre-attention decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "                \n",
        "        # shift right to insert start-of-sentence token and implement\n",
        "        # teacher forcing during training\n",
        "        tl.ShiftRight(),\n",
        "\n",
        "        # run an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(target_vocab_size, d_model),\n",
        "\n",
        "        # feed to an LSTM layer\n",
        "        tl.LSTM(d_model)        \n",
        "    )\n",
        "    \n",
        "    return pre_attention_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMXpNCLBOZRY"
      },
      "source": [
        "Prepping the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pmx0R1RObY2"
      },
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
        "    \n",
        "    Args:\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
        "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
        "    \n",
        "    Returns:\n",
        "        queries, keys, values and mask for attention.\n",
        "    \"\"\"    \n",
        "    \n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "    \n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "    \n",
        "    # generate the mask to distinguish real tokens from padding    \n",
        "    mask = ~fastnp.equal(inputs, 0)    \n",
        "    \n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    \n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].    \n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "        \n",
        "    \n",
        "    return queries, keys, values, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtuEp60_QE0S"
      },
      "source": [
        "Implementing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8vGr5ROQGrY"
      },
      "source": [
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\n",
        "\n",
        "    Args:\n",
        "    input_vocab_size: int: vocab size of the input\n",
        "    target_vocab_size: int: vocab size of the target\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "    n_attention_heads: int: number of attention heads\n",
        "    attention_dropout: float, dropout for the attention layer\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "    A LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "      \n",
        "    # Creating layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)    \n",
        "    # Creating layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)    \n",
        "    # Creating a serial network\n",
        "    model = tl.Serial( \n",
        "        \n",
        "      # Copying input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0,1,0,1]),        \n",
        "        \n",
        "      # Running input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "        \n",
        "      # Prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "        \n",
        "      # Running the AttentionQKV layer      \n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "      \n",
        "      # Drop the attention mask (i.e. index = None)\n",
        "      tl.Select([0,2]),\n",
        "        \n",
        "      # Running the rest of the RNN decoder\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
        "        \n",
        "      # Preparing output by making it the right size\n",
        "      tl.Dense(target_vocab_size),\n",
        "              \n",
        "      tl.LogSoftmax()\n",
        "    )        \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nevyi5JgQMD2"
      },
      "source": [
        "model = NMTAttn()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PSjxzCwQxpX"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0XlgZ4HQYwy"
      },
      "source": [
        "train_task = training.TrainTask(        \n",
        "        \n",
        "    labeled_data= train_batch_stream,\n",
        "        \n",
        "    loss_layer= tl.CrossEntropyLoss(),\n",
        "    \n",
        "    # Adam optimizer with learning rate of 0.01\n",
        "    optimizer= trax.optimizers.adam.Adam(0.01),\n",
        "        \n",
        "    # 1000 warmup steps with a max value of 0.01\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000,0.01),\n",
        "    \n",
        "    # Checkpoint every 10 steps\n",
        "    n_steps_per_checkpoint= 10,      \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFIxRC-ZQ7ss",
        "outputId": "59196ccf-68a8-4e88-8ae6-bc74d4663b82"
      },
      "source": [
        "eval_task = training.EvalTask(\n",
        "        \n",
        "    labeled_data=eval_batch_stream,\n",
        "        \n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:T5 library uses PAD_ID=0, which is different from the sentencepiece vocabulary, which defines pad_id=-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYpbBgKQ-Wg"
      },
      "source": [
        "# define the output directory\n",
        "output_dir = 'output_dir/'\n",
        "\n",
        "# remove old model if it exists. restarts training.\n",
        "!rm -f ~/output_dir/model.pkl.gz  \n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpGA82yfvIra"
      },
      "source": [
        "The training depends a lot on the dataset that is used. Of course a larger dataset means better training. If your dataset doesn't have a lot of conversations then it could be inaccurate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mOBKM7FRIYC"
      },
      "source": [
        "training_loop.run(50) #Experiment with 20, 50, 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt70PE6QT5D9"
      },
      "source": [
        "## Decoding the output\n",
        "\n",
        "Here we create some helper functions to help decode the output from the model and format the data as a chatbot. We use a greedy approach to predict the next words given a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bigAptgET6PD"
      },
      "source": [
        "# instantiate the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "model.init_from_file(\"output_dir/model.pkl.gz\", weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u768VNxuUVSB"
      },
      "source": [
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "\n",
        "    Args:\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"    \n",
        "    \n",
        "    token_length = len(cur_output_tokens)\n",
        "    \n",
        "    padded_length = int(np.ceil(2**np.log2(token_length + 1)))\n",
        "    \n",
        "    padded = cur_output_tokens + [0]*(padded_length - token_length)\n",
        "    \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # `padded` list is converted to a numpy array with shape (x, <padded_length>) where the\n",
        "    # x position is the batch axis.\n",
        "    padded_with_batch = np.expand_dims(np.array(padded), axis=0)\n",
        "    print()\n",
        "    # get the model prediction. remember to use the `NMTAttn` argument defined above.    \n",
        "    output, _ = NMTAttn((input_tokens,padded_with_batch))\n",
        "    \n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0,-1,:]\n",
        "\n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))        \n",
        "\n",
        "    return symbol, float(log_probs[symbol])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9SzxuDyUep6"
      },
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, vocab_type=None):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"        \n",
        "    \n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir, vocab_type=vocab_type)\n",
        "    \n",
        "    # initialize the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "    \n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0\n",
        "        \n",
        "    EOS = 1\n",
        "    print(\"Input tokens to sampling decode\",input_tokens)\n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS:\n",
        "        \n",
        "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "        \n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "    print(\"Output tokens to sampling decode\",cur_output_tokens)\n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir, vocab_type=vocab_type)    \n",
        "    \n",
        "    return cur_output_tokens, log_prob, sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enpebZ9wUyHN"
      },
      "source": [
        "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None, vocab_type=None):\n",
        "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
        "\n",
        "    Args:\n",
        "        sentence (str): a custom string.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    print(sentence)\n",
        "    while sentence != \"BYE\":      \n",
        "      _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir, vocab_type=vocab_type)\n",
        "      print(\"Chatbot: \" + translated_sentence)\n",
        "      sentence = input()        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uj0JT8lwC3f"
      },
      "source": [
        "Final chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u8AECBjVdWJ"
      },
      "source": [
        "initial_input_sentence= 'How are you doing today'\n",
        "\n",
        "greedy_decode_test(initial_input_sentence model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, vocab_type=VOCAB_TYPE);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}